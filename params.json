{
  "name": "Oracle-to-redshift-data-loader",
  "tagline": "Oracle-To-Redshift-Data-Loader",
  "body": "# Oracle-to-Redshift-Data-Loader\r\n    Used for ad-hoc query data results load from Oracle to Amazon-Redshift.\r\n    Works from Windows CLI (command line).\r\n\r\nFeatures:\r\n - Loads Oracle table (or query) data to Amazon-Redshift.\r\n - No need to create CSV extracts and S3 uploads before load to Redshift.\r\n - Data stream is compressed while loaded to S3 (and then to Redshift).\r\n - Works from your OS Windows desktop (command line).\r\n - It's executable (Oracle_To_Redshift_Loader.exe)  - no need for Python install.\r\n - It's 64 bit - it will work on any vanilla DOS for 64-bit Windows.\r\n - AWS Access Keys are not passed as arguments.  \r\n - You can modify default Python [extractor](https://github.com/alexbuz/Oracle-To-Redshift-Data-Loader/blob/master/dist-64bit/include/extractor.py) and [loader](https://github.com/alexbuz/Oracle-To-Redshift-Data-Loader/blob/master/dist-64bit/include/loader.py) code.\r\n - Written using Python/boto/psycopg2/PyInstaller.\r\n\r\n\r\n##Version\r\n\r\nOS|Platform|Version \r\n---|---|---- | -------------\r\nWindows|64bit|[1.2 beta]\r\n\r\n##Purpose\r\n\r\n- Stream/pipe/load Oracle table data to Amazon-Redshift.\r\n\r\n## How it works\r\n- Tool connects to source Oracle DB and opens data pipe for reading.\r\n- Data stream is compressed and pumped to S3 using multipart upload.\r\n- Optional upload to Reduced Redundancy storage (not RR by default).\r\n- Optional \"make it public\" after upload (private by default).\r\n- If S3 bucket doesn't exists it will be created.\r\n- You can control the region where new bucket is created.\r\n- Streamed data can be tee'd (dumped on disk) during load.\r\n- If not set, S3 Key defaulted to input query file name.\r\n- Data is loaded to Redshift from S3 using COPY command\r\n- Target Redshift table has to exist\r\n- It's a Python/boto/psycopg2 script\r\n\t* Boto S3 docs: http://boto.cloudhackers.com/en/latest/ref/s3.html\r\n\t* psycopg2 docs: http://initd.org/psycopg/docs/\r\n- Executable is created using [pyInstaller] (http://www.pyinstaller.org/)\r\n\r\n##Audience\r\n\r\nDatabase/ETL developers, Data Integrators, Data Engineers, Business Analysts, AWS Developers, SysOps\r\n\r\n##Designated Environment\r\nPre-Prod (UAT/QA/DEV)\r\n\r\n##Usage\r\n\r\n```\r\nc:\\Python35-32\\PROJECTS\\Ora2redshift>dist\\oracle_to_Redshift_loader.exe\r\n#############################################################################\r\n#Oracle-to-Redshift Data Loader (v1.2, beta, 04/05/2016 15:11:53) [64bit]\r\n#Copyright (c): 2016 Alex Buzunov, All rights reserved.\r\n#Agreement: Use this tool at your own risk. Author is not liable for any damages\r\n#           or losses related to the use of this software.\r\n################################################################################\r\nUsage:\r\n  set AWS_ACCESS_KEY_ID=<you access key>\r\n  set AWS_SECRET_ACCESS_KEY=<you secret key>\r\n\r\n  set ORACLE_LOGIN=tiger/scott@orcl\r\n  set ORACLE_CLIENT_HOME=C:\\app\\oracle12\\product\\12.1.0\\dbhome_1\r\n  \r\n  set NLS_DATE_FORMAT=\"MM/DD/YYYY HH12:MI:SS\"\r\n  set NLS_TIMESTAMP_FORMAT=\"MM/DD/YYYY HH12:MI:SS.FF\"\r\n  set NLS_TIMESTAMP_TZ_FORMAT=\"MM/DD/YYYY HH12:MI:SS.FF TZH:TZM\"\r\n\r\n  set REDSHIFT_CONNECT_STRING=\"dbname='***' port='5439' user='***' password='***' host='mycluster.***.redshift.amazonaws.com'\"\r\n\r\n\r\n  oracle_to_redshift_loader.exe [<ora_query_file>] [<ora_col_delim>] [<ora_add_header>]\r\n                            [<s3_bucket_name>] [<s3_key_name>] [<s3_use_rr>] [<s3_public>]\r\n\r\n        --ora_query_file -- SQL query to execure in source Oracle db.\r\n        --ora_col_delim  -- CSV column delimiter for downstream(,).\r\n        --ora_quote     -- Enclose values in quotes (\")\r\n        --ora_add_header -- Add header line to CSV file (False).\r\n        --ora_lame_duck  -- Limit rows for trial upload (1000).\r\n\r\n        --create_data_dump -- Use it if you want to persist streamed data on your filesystem.\r\n\r\n        --s3_bucket_name -- S3 bucket name (always set it).\r\n        --s3_location    -- New bucket location name (us-west-2)\r\n                                Set it if you are creating new bucket\r\n        --s3_key_name    -- CSV file name (to store query results on S3).\r\n                if <s3_key_name> is not specified, the oracle query filename (ora_query_file) will be used.\r\n        --s3_use_rr -- Use reduced redundancy storage (False).\r\n        --s3_write_chunk_size -- Chunk size for multipart upoad to S3 (10<<21, ~20MB).\r\n        --s3_public -- Make uploaded file public (False).\r\n\r\n        --red_to_table  -- Target Amazon-Redshit table name.\r\n        --red_col_delim  -- CSV column delimiter for upstream(,).\r\n        --red_quote     -- Set it if input values are quoted.\r\n        --red_timeformat -- Timestamp format for Redshift ('MM/DD/YYYY HH12:MI:SS').\r\n        --red_ignoreheader -- skip header in input stream\r\n\r\n        Oracle data uploaded to S3 is always compressed (gzip).\r\n\r\n        Boto S3 docs: http://boto.cloudhackers.com/en/latest/ref/s3.html\r\n        psycopg2 docs: http://initd.org/psycopg/docs/\r\n\r\n```\r\n#Example\r\n\r\n\r\n###Environment variables\r\nSet the following environment variables (for all tests):\r\n\r\n```\r\nset AWS_ACCESS_KEY_ID=<you access key>\r\nset AWS_SECRET_ACCESS_KEY=<you secret key>\r\n\r\nset ORACLE_LOGIN=tiger/scott@orcl\r\nset ORACLE_CLIENT_HOME=C:\\\\app\\\\oracle12\\\\product\\\\12.1.0\\\\dbhome_1\r\n\r\n  set NLS_DATE_FORMAT=\"MM/DD/YYYY HH12:MI:SS\"\r\n  set NLS_TIMESTAMP_FORMAT=\"MM/DD/YYYY HH12:MI:SS.FF\"\r\n  set NLS_TIMESTAMP_TZ_FORMAT=\"MM/DD/YYYY HH12:MI:SS.FF TZH:TZM\"\r\n  \r\nset REDSHIFT_CONNECT_STRING=\"dbname='***' port='5439' user='***' password='***' host='mycluster.***.redshift.amazonaws.com'\"  \r\n```\r\n\r\n### Test load with data dump.\r\nOracle table `crime_test` contains data from data.gov [Crime](https://catalog.data.gov/dataset/crime) dataset.\r\nIn this example complete table `crime_test` get's uploaded to Aamzon-S3 as compressed CSV file.\r\n\r\nContents of the file *table_query.sql*:\r\n\r\n```\r\nSELECT * FROM crime_test;\r\n\r\n```\r\nAlso temporary dump file is created for analysis (by default there are no files created)\r\nUse `-s, --create_data_dump` to dump streamed data.\r\n\r\nIf target bucket does not exists it will be created in user controlled region.\r\nUse argument `-t, --s3_location` to set target region name\r\n\r\nContents of the file *test.bat*:\r\n```\r\ndist-64bit\\oracle_to_redshift_loader.exe ^\r\n-q table_query.sql ^\r\n-d \",\" ^\r\n-b test_bucket ^\r\n-k oracle_table_export ^\r\n-r ^\r\n-o crime_test ^\r\n-m \"DD/MM/YYYY HH12:MI:SS\" ^\r\n-s\r\n```\r\nExecuting `test.bat`:\r\n\r\n```\r\nc:\\Python35-32\\PROJECTS\\Ora2redshift>dist-64bit\\oracle_to_redshift_loader.exe -q table_query.sql -d \",\" -b test_bucket -k oracle_table_export -r -o crime_test -m \"DD/MM/YYYY HH12:MI:SS\" -s\r\nUploading results of \"table_query.sql\" to existing bucket \"test_bucket\"\r\nStarted reading from Oracle (1.25 sec).\r\nDumping data to: c:\\Python35-32\\PROJECTS\\Ora2redshift\\data_dump\\table_query\\test_bucket\\oracle_table_export.20160408_203221.gz\r\n1 chunk 10.0 MB [11.36 sec]\r\n2 chunk 10.0 MB [11.08 sec]\r\n3 chunk 10.0 MB [11.14 sec]\r\n4 chunk 10.0 MB [11.12 sec]\r\n5 chunk 877.66 MB [0.96 sec]\r\nSize: Uncompressed: 40.86 MB\r\nSize: Compressed  : 8.95 MB\r\nElapsed: Oracle+S3    :69.12 sec.\r\nElapsed: S3->Redshift :3.68 sec.\r\n--------------------------------\r\nTotal elapsed: 72.81 sec.\r\n\r\n\r\n```\r\n\r\n![test](https://raw.githubusercontent.com/alexbuz/Oracle-To-Redshift-Data-Loader/master/test/ora2redshift.png)\r\n\r\n### Modifying default Redshift COPY command.\r\nYou can modify default Redshift COPY command this script is using.\r\n\r\nOpen file [include\\loader.py](https://github.com/alexbuz/Oracle-To-Redshift-Data-Loader/blob/master/dist-64bit/include/loader.py) and modify `sql` variable on line 24.\r\n\r\n```\r\n\tsql=\"\"\"\r\nCOPY %s FROM '%s' \r\n\tCREDENTIALS 'aws_access_key_id=%s;aws_secret_access_key=%s' \r\n\tDELIMITER '%s' \r\n\tFORMAT CSV %s \r\n\tGZIP \r\n\t%s \r\n\t%s; \r\n\tCOMMIT;\r\n\t...\r\n```\r\n\r\n\r\n###Download\r\n* `git clone https://github.com/alexbuz/Oracle-to-Redshift-Data-Loader`\r\n* [Master Release](https://github.com/alexbuz/Oracle-to-Redshift-Data-Loader/archive/master.zip) -- `oracle_to_redshift_loader 1.2`\r\n\r\n\r\n\r\n\r\n#\r\n#\r\n#\r\n#\r\n#   \r\n#FAQ\r\n#  \r\n#### Can it load Oracle data to Amazon Redshift Database?\r\nYes, it is the main purpose of this tool.\r\n\r\n#### Can developers integrate `Oracle-to-Redshift-Data-Loader` into their ETL pipelines?\r\nYes. Assuming they are doing it on OS Windows.\r\n\r\n#### How fast is data load using `Oracle-to-Redshift-Data-Loader`?\r\nAs fast as any implementation of multi-part load using Python and boto.\r\n\r\n####How to inscease load speed?\r\nInput data stream is getting compressed before upload to S3. So not much could be done here.\r\nYou may want to run it closer to source or target endpoints for better performance.\r\n\r\n#### What are the other ways to move large amounts of data from Oracle to Redshift?\r\nYou can write a sqoop script that can be scheduled with Data Pipeline.\r\n\r\n#### Does it create temporary data file?\r\nNo\r\n\r\n#### Can I log transfered data for analysis?\r\nYes, Use `-s, --create_data_dump` to dump streamed data.\r\n\r\n#### Explain first step of data transfer?\r\nThe query file you provided is used to select data form target Oracle server.\r\nStream is compressed before load to S3.\r\n\r\n#### Explain second step of data transfer?\r\nCompressed data is getting uploaded to S3 using multipart upload protocol.\r\n\r\n#### Explain third step of data load. How data is loaded to Amazon Redshift?\r\nYou Redshift cluster has to be open to the world (accessible via port 5439 from internet).\r\nIt uses PostgreSQL COPY command to load file located on S3 into Redshift table.\r\n\r\n#### What technology was used to create this tool\r\nI used SQL*Plus, Python, Boto to write it.\r\nBoto is used to upload file to S3. \r\nSQL*Plus is used to spool data to compressor pipe.\r\npsycopg2 is used to establish ODBC connection with Redshift clusted and execute `COPY` command.\r\n\r\n#### Why don't you use ODBC driver for Redshift to insert data?\r\nFrom my experience it's much slower that COPY command.\r\nIt's 10x faster to upload CSV file to Amazon-S3 first and then run COPY command.\r\nYou can still use ODBC for last step.\r\nIf you are a Java shop, take a look at Progress [JDBC Driver](https://www.progress.com/blogs/booyah-amazon-redshift-challenge-loaded-in-8-min-oow14).\r\nThey claim it can load 1 mil records in 6 min.\r\n\r\n\r\n#### What would be my Oracle-to-AWS migration strategy?\r\n - Size the database\r\n - Network\r\n - Version of Oracle\r\n - Oracle clinet (SQL*Plus) availability\r\n - Are you doing it in one step or multiple iterations?\r\n \r\n#### Was there an AWS white paper on Oracle to AWS migration strategies?\r\nYes, [here](https://d0.awsstatic.com/whitepapers/strategies-for-migrating-oracle-database-to-aws.pdf) it is.\r\n\r\n#### Do you use psql to execute COPY command against Redshift?\r\nNo. I use `psycopg2` python module (ODBC).\r\n\r\n#### Why are you uploading extracted data to S3? whould it be easier to just execute COPY command for local spool file?\r\nAs of now you cannot load from local file. You can use COPY command with Amazon Redshift, but only with files located on S3.\r\nIf you are loading CSV file from Windows command line - take a look at [CSV_Loader_For_Redshift](https://github.com/alexbuz/CSV_Loader_For_Redshift)\r\n\r\n#### Can I modify spooler code?\r\nYes. Spooler code is in [include\\extractor.py](https://github.com/alexbuz/Oracle-To-Redshift-Data-Loader/blob/master/dist-64bit/include/extractor.py).\r\n\r\n#### Can I modify default psql COPY command?\r\nYes. Edit [include\\loader.py](https://github.com/alexbuz/Oracle-To-Redshift-Data-Loader/blob/master/dist-64bit/include/loader.py) and add/remove COPY command options\r\n\r\nOther options you may use:\r\n\r\n    COMPUPDATE OFF\r\n    EMPTYASNULL\r\n    ACCEPTANYDATE\r\n    ACCEPTINVCHARS AS '^'\r\n    GZIP\r\n    TRUNCATECOLUMNS\r\n    FILLRECORD\r\n    DELIMITER '$DELIM'\r\n    REMOVEQUOTES\r\n    STATUPDATE ON\r\n    MAXERROR AS $MaxERROR\r\n\r\n#### Does it delete file from S3 after upload?\r\nNo\r\n\r\n#### Does it create target Redshift table?\r\nBy default no, but using [include\\loader.py](https://github.com/alexbuz/Oracle-To-Redshift-Data-Loader/blob/master/dist-64bit/include/loader.py) you can extend default functionality and code in target table creation.\r\n\r\n#### Can I execute multiple loaders in parallel?\r\nYes. Use Windows CLI Powershell to schedule multiple loaders in parallel.\r\n\r\nCan I delete/truncate Oracle source after load?\r\nNo, but you can code it into [include\\extractor.py](https://github.com/alexbuz/Oracle-To-Redshift-Data-Loader/blob/master/dist-64bit/include/extractor.py)\r\n\r\n#### Where are the sources?\r\nPlease, contact me for sources.\r\nExtractor code: [include\\extractor.py](https://github.com/alexbuz/Oracle-To-Redshift-Data-Loader/blob/master/dist-64bit/include/extractor.py)\r\nLoader code: [include\\loader.py](https://github.com/alexbuz/Oracle-To-Redshift-Data-Loader/blob/master/dist-64bit/include/loader.py)\r\n\r\n#### Can you modify functionality and add features?\r\nYes, please, ask me for new features.\r\n\r\n#### What other AWS tools you've created?\r\n- [Oracle_To_S3_Data_Uploader] (https://github.com/alexbuz/Oracle_To_S3_Data_Uploader) - Stream Oracle data to Amazon- S3.\r\n- [CSV_Loader_For_Redshift] (https://github.com/alexbuz/CSV_Loader_For_Redshift/blob/master/README.md) - Append CSV data to Amazon-Redshift from Windows.\r\n- [S3_Sanity_Check] (https://github.com/alexbuz/S3_Sanity_Check/blob/master/README.md) - let's you `ping` Amazon-S3 bucket to see if it's publicly readable.\r\n- [EC2_Metrics_Plotter](https://github.com/alexbuz/EC2_Metrics_Plotter/blob/master/README.md) - plots any CloudWatch EC2 instance  metric stats.\r\n- [S3_File_Uploader](https://github.com/alexbuz/S3_File_Uploader/blob/master/README.md) - uploads file from Windows to S3.\r\n\r\n#### Do you have any AWS Certifications?\r\nYes, [AWS Certified Developer (Associate)](https://raw.githubusercontent.com/alexbuz/FAQs/master/images/AWS_Ceritied_Developer_Associate.png)\r\n\r\n#### Can you create similar/custom data tool for our business?\r\nYes, you can PM me here or email at `alex_buz@yahoo.com`.\r\nI'll get back to you within hours.\r\n\r\n###Links\r\n - [Employment FAQ](https://github.com/alexbuz/FAQs/blob/master/README.md)\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}